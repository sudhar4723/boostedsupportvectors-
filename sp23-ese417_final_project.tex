\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{ESE 417 Final Project}

\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother

\author{\IEEEauthorblockN{1\textsuperscript{st} Myan Sudharsanan}
\IEEEauthorblockA{\textit{Computer Science and  Engineering Dept.} \\
\textit{Washington University in St. Louis}\\
St. Louis, MO \\
m.s.sudharsanan@wustl.edu}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Reedham Kalariya}
\IEEEauthorblockA{\textit{Computer Science and Engineering Dept.} \\
\textit{Washington University in St. Louis}\\
St. Louis, MO \\
j.kalariya@wustl.edu}
\linebreakand
\IEEEauthorblockN{3\textsuperscript{rd} Jeremy Stiava}
\IEEEauthorblockA{\textit{Computer Science and Engineering Dept.} \\
\textit{Washington University in St. Louis}\\
St. Louis, MO \\
email address or ORCID}
}

\maketitle

\begin{abstract}
Your abstract should consist of a single paragraph up to 250 words, with correct grammar and unambiguous terminology. Provide a concise summary of the project. Include the conclusions reached and the potential implications of those conclusions. It should be self-contained -- no abbreviations, footnotes, references, or mathematical equations. It also should highlight what is unique in your work.
\end{abstract}


\section{Introduction}
We were given the red wine dataset from the UCI Machine Learning Library to explore. According to the library’s website, the red wine set is actually a subset of data that also consists of white wine. Both the red and white variants consist of Vinho Verde wine. Features of this dataset include fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol, and quality (scored between 0 and 10). As the site had suggested, we treated the quality column as the “y” or output while the remaining features are regarded as part of our “x” or input set. However, before we could pass in that entire input set, we wanted to attempt some data wrangling. Upon initial observations, we identified very low classification scores for each model, hence we wanted to see if we could manipulate the dataset in some sense. In fact, the site even suggests we utilize outlier detection algorithms to sort out the good wines from the bad ones. We also came to the realization that certain columns may not be very relevant for our classifications which we dealt with either by dropping or combining. After this data manipulation however, we came to the conclusion that…

\section{Methods}
Our first target was to understand the nature of the given data. First, as starter code, we employed the tactic of loading the dataset into a given dataframe, and separating the aforementioned features from the output. We then plugged in our data into the ANN model, the SVM model, and the Random Forest model, to get a rough idea of the weaknesses of this dataset. We found very low classification scores between 0.5 and 0.6, so we backtracked to our dataset to figure what may be causing these low scores. As a general rule of thumb it is best practice to retain columns with high variance and drop the columns with relatively low variance in the context of the given statistic. For the former case, columns with high variance indicate the true ranges for each statistic in a dataset, so they are vital for establishing a strong classification score. However, they do present the problem of dealing with outliers, since outliers can either heavily inflate or deflate a classification score. Before we can deal with outliers, however, we need to work with the extraneous data. For the latter case of dropping columns, we noticed columns such as pH and sulphates have very low variance, hence can be dropped as it will not contribute significantly to the classification score. Additionally, a column with low variance probably indicates that it is not an essential ingredient of wine in terms of affecting the quality, given all of the wine had similar values. In order to drop these columns, we use the pandas dataframe function “drop” and convert the returned multi-dimensional array back to a pandas dataframe. This process is described…

The curse of dimensionality is the problem regarding the use of several dimensions of data to identify a certain classification. Hence, we believed this necessitated the use of a dimensionality reduction algorithm to alleviate this issue. One such algorithm is PCA, or principal component analysis. This is quite similar to 

Having dealt with extraneous data, we wanted to integrate the high variance columns into our models without the outliers greatly affecting the classification score. We went about doing so by standardizing the data. Instead of selecting min and max values to scale by, we set the mean for each column to be zero with a standard deviation of one, such that the data in each column is normally distributed. By centering the data in such a manner, we are minimizing the negative impact of outliers, while also retaining a normal distribution within each column. We achieved this by using the sklearn.preprocessing.StandardScaler package, then fitting and transforming to our modified dataframe (which we have dropped columns from). However, this yielded poor results, as we will discuss in the following section. The implementation follows as below…

The second model we tackled was KNearestNeighborsClassifier, or KNN for brevity. Essentially, in KNN, you calculate a distance value relative to test points, which serves as your predicted value. Following that, we sort the predicted values in ascending order. Then, with a given value of K, we pick the first K elements of the set. Within this subset, we then select the mode classification of the subset, or in other words, the most frequent classification within the subset, which will subsequently become the classification for the test point.

We then proceeded with the process of cross-validation and hyperparameter tuning. Starting with the former, cross-validation is the process used to avoid overfitting of the data, with our specific cross-validator being KFold cross-validation.	
KFold cross-validation involves splitting the dataset into K equally sized subsets of the testing set and using each fold as the test set once while training the model on the remaining folds. Note that our final implementation does not include KFolds, due to massive runtimes for when we used KFold. Once we have completed cross-validation, we move on to hyperparameter tuning, which is essentially the process of modifying several different parameters of the classifiers to find the optimal parameters to give us the best accuracy. 
For example, for our random forest model, we modify the number of estimators,
the criterion, the \emph{max\_depth}, \emph{max\_features}, and \emph{class\_weight}. In order to run through repeated iterations of changing parameters, we used the \emph{GridSearchCV} function within \emph{sklearn.model\_selection}.

% \subsection{Understanding the SIRD Model}
% The basic concept of the SIRD model revolves around linear dynamical systems. Essentially, these are systems that rely on states (or time periods in this context) to progress dynamic values represented in terms of a matrix, as seen below, with $t$ representing time (in days in our context):
% \begin{equation}
%     x_{t+1}=A_{t}x_{t} \label{eq}
% \end{equation}
% Here, $A$ is denoted as a dynamic matrix, given that they provide the parameters that progress the model or modify the following vectors, and $x_t$ and $x_{t+1}$ form the vectors that define the states or periods of time for certain groups. From this equation, we can clearly see that linear dynamical systems can be used as predictive tool for modeling a certain scenario, like our COVID pandemic scenario. This relates to a special variant of linear dynamical system that we employ is SIRD, which stands for Susceptible, Infected, Recovered, and Deceased, respectively. Given that we have the four different conditions within a period of time, we find that $x_t$ and $x_{t+1}$ to be 4-vectors, and $A$ to be a 4x4 matrix representing the percentages of a population within each respective SIRD group.
% 	We decided to model this phenomena outlined in the textbook by creating our own dynamic matrix with a starting input and observing the trends of the data over time and size of each group. We defined our dynamic matrix as the following (FIX THIS PLEASE MYAN):
% \begin{equation}
%     x_{t+1}=A_{t}x_{t} \label{eq}
% \end{equation}
% This is the same matrix as in the textbook, and hence we tried to replicate with Figure (INSERT FIGURE IN FIGURE SECTION), with similar definitions of the columns as stated in the textbook. The visualization in plots and figure section will provide greater detail as to the change in states over time. However, we wanted to expand upon the textbook's definition of SIRD and explore the possibility of using SIRD to model re-infections. We defined a new matrix as the following (FIX THIS PLEASE MYAN):
% \begin{equation}
%     x_{t+1}=A_{t}x_{t} \label{eq}
% \end{equation}
% Now that we have some additional values, let's take a look what makes this dynamic matrix account for re-infection. Note that similar to the last matrix, the fourth column consists solely of the unit vector in the fourth dimension, due to the fact that if you reach the death state, it is impossible to reach another state from there. In the first column, observe that 97\% of the susceptible population will remain susceptible, while 2\% of the susceptible population will get infected, and 1\% of the susceptible population will recover. In the following column we find 4\% of the infected population will recover from the illness but remain susceptible, 86\% will continue to be infected, 9\% of the infected population will recover, and finally 1\% of the infected population will die from the illness. In our last column of relevance we find that 20\% of the recovered people end up getting re-infected (hence the modeling of re-infection) and 80\% of the recovered people actually remain recovered. Further analysis of this model can be seen in the plots and figures section. An important item to note in this last model is that we cannot make the assumption that once recovered, one can reach immunity, given that there is a possibility of re-infection. Now that we have both understood and built and SIRD model, we can begin to apply it to the given COVID dataset.

% \subsection{Some Common Mistakes}\label{SCM}
% Delete this subsection once you read it.
% \begin{itemize}
% \item The word ``data'' is plural, not singular.
% \item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
% \item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
% \item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
% \item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
% \item Do not confuse ``imply'' and ``infer''.
% \item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
% \item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
% \end{itemize}
% An excellent style manual for science writers is \cite{b7}.


% \subsection{Figures and Tables}
% Delete this subsection once you read it.

% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Figure captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. Use the abbreviation 
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% \begin{figure}[htbp]
% % \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
% rather than symbols or abbreviations when writing Figure axis labels to 
% avoid confusing the reader. As an example, write the quantity 
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
% units in the label, present them within parentheses. Do not label axes only 
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
% quantities and units. For example, write ``Temperature (K)'', not 
% ``Temperature/K''.

% \section{Results and Discussion}
% Show the results that you achieved in your work and offer an interpretation of those results. Acknowledge any limitations of your work and avoid exaggerating the importance of the results.

% \section{Conclusion}
% Summarize your key findings. Include important conclusions that can be drawn. Discuss benefits or shortcomings of your work and suggest future related project ideas you might like to explore in the future.

% \begin{thebibliography}{00}
% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \end{thebibliography}


\end{document}
